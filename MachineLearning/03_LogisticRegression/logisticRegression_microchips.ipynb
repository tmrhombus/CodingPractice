{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69132e04",
   "metadata": {},
   "source": [
    "# Logistic Regression : Microchip Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a44d03",
   "metadata": {},
   "source": [
    "## Read in data, make X, y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e92ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b637989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/ng_microchips.csv\", header=None)\n",
    "#df.head()\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0b8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1].values # all rows, all columns except last\n",
    "y=df.iloc[:,-1].values  # all rows, last column\n",
    "\n",
    "# print(y)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd466ae",
   "metadata": {},
   "source": [
    "## Plot the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92cca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not accepted\n",
    "marker0_type  = \"o\" \n",
    "marker0_color = \"yellow\"\n",
    "marker0_face  = \"yellow\"\n",
    "marker0_edge  = \"black\"\n",
    "marker0_size  = 18\n",
    "\n",
    "# accepted \n",
    "marker1_type  = \"+\" \n",
    "marker1_color = \"black\"\n",
    "marker1_face  = \"black\"\n",
    "marker1_edge  = \"black\"\n",
    "marker1_size  = 18\n",
    "\n",
    "xmin = -1.\n",
    "xmax = 1.2\n",
    "ymin = -1.\n",
    "ymax = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9c3add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos , neg = (y==1).reshape(118,1) , (y==0).reshape(118,1)\n",
    "plt.scatter(X[pos[:,0],0],X[pos[:,0],1],\n",
    "            c=marker1_color,\n",
    "            marker=marker1_type,\n",
    "            s=marker1_size,\n",
    "            label=\"Passed\")\n",
    "plt.scatter(X[neg[:,0],0],X[neg[:,0],1],\n",
    "            marker=marker0_type,\n",
    "            color=marker0_color,\n",
    "            facecolors=marker0_face,\n",
    "            edgecolors=marker0_edge,\n",
    "            s=marker0_size,\n",
    "            label=\"Failed\"\n",
    "            )\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.xlim([xmin,xmax])\n",
    "plt.ylim([ymin,ymax])\n",
    "\n",
    "plt.title(\"Accept/Reject Microchips Based on Two Tests\")\n",
    "plt.savefig(\"./output/microchips_initialdata.png\", facecolor=\"w\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0023cd",
   "metadata": {},
   "source": [
    "## Feature Mapping\n",
    "combine a and b as new columns of matrix as:\n",
    "$ 1 | a | b | a^2 | ab | b^2 | a^3 | a^2 b | a b^2 | b^3 |$  etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa77181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(x1,x2,degree):\n",
    "    \"\"\"\n",
    "    take in numpy array of x1 and x2, return all polynomial terms up to the given degree\n",
    "    \"\"\"\n",
    "    out = np.ones(len(x1)).reshape(len(x1),1) # reshape to make a column vector\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            terms= (x1**(i-j) * x2**j).reshape(len(x1),1)\n",
    "            out= np.hstack((out,terms))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6512a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 28)\n"
     ]
    }
   ],
   "source": [
    "X = mapFeature(X[:,0], X[:,1],6) # convert X matrix \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08628",
   "metadata": {},
   "source": [
    "## Regularized Cost Function and Gradient\n",
    "\n",
    "$m = $ number of training examples \\\n",
    "$n = $ number of parameters ($\\exists$ a $\\theta_0$)\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^T x)$ = hypothesis function (of x given $\\theta$)\\\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$ = sigmoid function\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m \\left[ -y^{(i)}\\log\\left(h_\\theta (x^{(i)}) \\right) - (1-y^{(i)})\\log\\left(1-h_\\theta(x^{(i)})  \\right) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left[ h_\\theta\\left( x^{(i)} \\right)-y^{(i)} \\right]x_0^{(i)}$ for $j=0$\\\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left[ h_\\theta\\left( x^{(i)} \\right)-y^{(i)} \\right]x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j$ for $j\\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d6c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/ (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9068be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y ,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array of theta, X, and y to return the regularize cost function and gradient\n",
    "    of a logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    m=len(y)\n",
    "    y=y[:,np.newaxis]\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
    "    cost = 1/m * sum(error)\n",
    "    regCost= cost + Lambda/(2*m) * sum(theta**2)\n",
    "    \n",
    "    # compute gradient\n",
    "    j_0= 1/m * (X.transpose() @ (predictions - y))[0]\n",
    "    j_1 = 1/m * (X.transpose() @ (predictions - y))[1:] + (Lambda/m)* theta[1:]\n",
    "    grad= np.vstack((j_0[:,np.newaxis],j_1))\n",
    "    return regCost[0], grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c13dd",
   "metadata": {},
   "source": [
    "## Test Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73f3182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599461\n",
      "(28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Set regularization parameter lambda to 1\n",
    "Lambda = 1\n",
    "\n",
    "#Compute and display initial cost and gradient for regularized logistic regression\n",
    "cost, grad=costFunctionReg(initial_theta, X, y, Lambda)\n",
    "print(cost)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca30fb8",
   "metadata": {},
   "source": [
    "## Do Gradient Descent to find best Theta Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accbd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = costFunctionReg(theta,X,y,Lambda)\n",
    "        theta = theta - (alpha * grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f93a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 0.5, 1, 5, 100]\n",
    "thetas = []\n",
    "J_histories = []\n",
    "colors = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\"]\n",
    "\n",
    "for lam in lambdas:\n",
    "    \n",
    " theta, J_history = gradientDescent(X,y,initial_theta,1,800,lam)\n",
    " thetas.append(theta)\n",
    " J_histories.append(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cff4ed",
   "metadata": {},
   "source": [
    "## Plot Cost Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a3cadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for jh,lam, col in zip(J_histories, lambdas, colors):\n",
    " plt.plot(jh,label=r\"$\\lambda$ = {}\".format(lam), c=col)\n",
    " #plt.plot(J_histories[0])\n",
    " plt.xlabel(\"Iteration\")\n",
    " plt.ylabel(\"$J(\\Theta)$\")\n",
    " plt.title(\"Cost function using Gradient Descent\")\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.savefig(\"./output/microchips_J_convergence.png\", facecolor=\"w\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927a64d",
   "metadata": {},
   "source": [
    "## Plot Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb25c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeaturePlot(x1,x2,degree):\n",
    "    \"\"\"\n",
    "    take in numpy array of x1 and x2, return all polynomial terms up to the given degree\n",
    "    don't save it as an ndarray \n",
    "    \"\"\"\n",
    "    out = np.ones(1)\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            terms= (x1**(i-j) * x2**j)\n",
    "            out= np.hstack((out,terms))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814f5bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "plt.scatter(X[pos[:,0],1],X[pos[:,0],2],\n",
    "            c=marker1_color,\n",
    "            marker=marker1_type,\n",
    "            s=marker1_size,\n",
    "            label=\"Passed\")\n",
    "plt.scatter(X[neg[:,0],1],X[neg[:,0],2],\n",
    "            marker=marker0_type,\n",
    "            color=marker0_color,\n",
    "            facecolors=marker0_face,\n",
    "            edgecolors=marker0_edge,\n",
    "            s=marker0_size,\n",
    "            label=\"Failed\"\n",
    "            )\n",
    "# Plotting decision boundary\n",
    "\n",
    "u_vals = np.linspace(-1,1.5,50)\n",
    "v_vals= np.linspace(-1,1.5,50)\n",
    "z=np.zeros((len(u_vals),len(v_vals)))\n",
    "\n",
    "for lam,thet,col in zip(lambdas,thetas,colors):\n",
    "\n",
    " for i in range(len(u_vals)):\n",
    "     for j in range(len(v_vals)):\n",
    "         z[i,j] =mapFeaturePlot(u_vals[i],v_vals[j],6) @ thet\n",
    " \n",
    " CS = ax1.contour(u_vals,v_vals,z.T,0,colors=col)\n",
    " ax1.clabel(CS, inline=True, fmt=\"$\\lambda$={}\".format(lam), fontsize=10)\n",
    "\n",
    "plt.xlim([xmin,xmax])\n",
    "plt.ylim([ymin,ymax])\n",
    "\n",
    "plt.xlabel(\"Test 1\")\n",
    "plt.ylabel(\"Test 2\")\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.title(\"Logistic Regression to find Decision Boundary\")\n",
    "\n",
    "plt.savefig(\"./output/microchips_finalfits.png\", facecolor=\"w\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb53acc",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e4166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    take in numpy array of theta and X and predict the class \n",
    "    \"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    return predictions>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e72a939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Lambda = 0, Training Accuracy: 83.89830508474576\n",
      "For Lambda = 0.5, Training Accuracy: 82.20338983050848\n",
      "For Lambda = 1, Training Accuracy: 83.05084745762711\n",
      "For Lambda = 5, Training Accuracy: 81.35593220338984\n",
      "For Lambda = 100, Training Accuracy: 61.016949152542374\n"
     ]
    }
   ],
   "source": [
    "for thet,lam in zip(thetas,lambdas):\n",
    " \n",
    " p=classifierPredict(thet,X)\n",
    " print(\"For Lambda = {}, Training Accuracy: {}\".format(lam,(sum(p==y[:,np.newaxis])/len(y) *100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3ad20",
   "metadata": {},
   "source": [
    "## Convert to real python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d46d02d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook logisticRegression_microchips.ipynb to script\n",
      "[NbConvertApp] Writing 7378 bytes to logisticRegression_microchips.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script logisticRegression_microchips.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
